{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os \n",
    "import shutil\n",
    "from glob import glob\n",
    "pj = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layers_file = 'weights_layer4_tensorflow.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = h5py.File(all_layers_file, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0153b67b8838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisititems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_attrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "def print_attrs(name, obj):\n",
    "#     print(name)\n",
    "    for key, val in obj.attrs.iteritems():\n",
    "        print(\"    %s: %s %s\" % (key, val, type(val)))\n",
    "\n",
    "\n",
    "# f.visititems(print_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_file_newtf(fname):\n",
    "    filename = os.path.basename(fname)\n",
    "    path = os.path.dirname(fname)\n",
    "    name, ext = filename.split('.')\n",
    "    name += '_new'\n",
    "    return pj(path, '.'.join([name, ext]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    <HDF5 dataset \"batchnormalization_1_beta:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_1_gamma:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_1_running_mean:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_1_running_std:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_2_beta:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_2_gamma:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_2_running_mean:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_2_running_std:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_3_beta:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_3_gamma:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_3_running_mean:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_3_running_std:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_4_beta:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_4_gamma:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_4_running_mean:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_4_running_std:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_5_beta:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_5_gamma:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_5_running_mean:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"batchnormalization_5_running_std:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"convolution2d_1_W:0\": shape (32, 1, 3, 3), type \"<f4\">: (32, 1, 3, 3)\n",
      "    <HDF5 dataset \"convolution2d_1_b:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"convolution2d_2_W:0\": shape (32, 32, 3, 3), type \"<f4\">: (32, 32, 3, 3)\n",
      "    <HDF5 dataset \"convolution2d_2_b:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"convolution2d_3_W:0\": shape (32, 32, 3, 3), type \"<f4\">: (32, 32, 3, 3)\n",
      "    <HDF5 dataset \"convolution2d_3_b:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"convolution2d_4_W:0\": shape (32, 32, 3, 3), type \"<f4\">: (32, 32, 3, 3)\n",
      "    <HDF5 dataset \"convolution2d_4_b:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"convolution2d_5_W:0\": shape (32, 32, 3, 3), type \"<f4\">: (32, 32, 3, 3)\n",
      "    <HDF5 dataset \"convolution2d_5_b:0\": shape (32,), type \"<f4\">: (32,)\n",
      "    <HDF5 dataset \"Variable:0\": shape (512, 1, 1, 257), type \"<f4\">: (512, 1, 1, 257)\n",
      "    <HDF5 dataset \"Variable_1:0\": shape (512, 1, 1, 257), type \"<f4\">: (512, 1, 1, 257)\n",
      "    <HDF5 dataset \"Variable_2:0\": shape (257, 96), type \"<f4\">: (257, 96)\n"
     ]
    }
   ],
   "source": [
    "def print_shapes(name, obj):\n",
    "    if type(obj)== h5py.Dataset:# or type(obj) == h5py.Group:\n",
    "        return\n",
    "    for key, val in obj.items():\n",
    "        if 'bup' not in key:\n",
    "            print(\"    %s: %s\" % (val, val.shape))\n",
    "f = h5py.File(rename_file_newtf(all_layers_file), 'r')\n",
    "\n",
    "f.visititems(print_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ConvBNEluDr', u'flatten_1', u'melgram']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'batchnormalization_1_beta:0',\n",
       " u'batchnormalization_1_gamma:0',\n",
       " u'batchnormalization_1_running_mean:0',\n",
       " u'batchnormalization_1_running_std:0',\n",
       " u'batchnormalization_2_beta:0',\n",
       " u'batchnormalization_2_gamma:0',\n",
       " u'batchnormalization_2_running_mean:0',\n",
       " u'batchnormalization_2_running_std:0',\n",
       " u'batchnormalization_3_beta:0',\n",
       " u'batchnormalization_3_gamma:0',\n",
       " u'batchnormalization_3_running_mean:0',\n",
       " u'batchnormalization_3_running_std:0',\n",
       " u'batchnormalization_4_beta:0',\n",
       " u'batchnormalization_4_gamma:0',\n",
       " u'batchnormalization_4_running_mean:0',\n",
       " u'batchnormalization_4_running_std:0',\n",
       " u'batchnormalization_5_beta:0',\n",
       " u'batchnormalization_5_gamma:0',\n",
       " u'batchnormalization_5_running_mean:0',\n",
       " u'batchnormalization_5_running_std:0',\n",
       " u'convolution2d_1_W:0',\n",
       " u'convolution2d_1_b:0',\n",
       " u'convolution2d_2_W:0',\n",
       " u'convolution2d_2_b:0',\n",
       " u'convolution2d_3_W:0',\n",
       " u'convolution2d_3_b:0',\n",
       " u'convolution2d_4_W:0',\n",
       " u'convolution2d_4_b:0',\n",
       " u'convolution2d_5_W:0',\n",
       " u'convolution2d_5_b:0']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[u'ConvBNEluDr'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file(input_filename , rename_file_path):\n",
    "    f = h5py.File(input_filename, 'r')\n",
    "    print(rename_file_path)\n",
    "    if os.path.exists(rename_file_path):\n",
    "        os.remove(rename_file_path)\n",
    "    shutil.copyfile(input_filename, rename_file_path)\n",
    "\n",
    "    f_out = h5py.File(rename_file_path, 'r+')\n",
    "    def print_attrs(name, obj):\n",
    "        if type(obj) is h5py.Group:\n",
    "            return\n",
    "\n",
    "        group_name, val_name = name.split('/')\n",
    "\n",
    "        if type(obj) is h5py.Dataset and 'convolution' in name and '_W' in name:\n",
    "            f_out[group_name].move(val_name, val_name+'_bup')\n",
    "            obj = np.array(obj).transpose((0,2,3,1))\n",
    "            print(obj.shape)\n",
    "            f_out[group_name].update({val_name:  obj})\n",
    "    f.visititems(print_attrs)\n",
    "    f_out.close()\n",
    "# convert_file(all_layers_file, rename_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '.'\n",
    "files = glob(pj(directory, 'weights_layer[0-9]_tensorflow.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./weights_layer1_tensorflow_new.hdf5\n",
      "(32, 3, 3, 1)\n",
      "(32, 3, 3, 32)\n",
      "./weights_layer0_tensorflow_new.hdf5\n",
      "(32, 3, 3, 1)\n",
      "./weights_layer2_tensorflow_new.hdf5\n",
      "(32, 3, 3, 1)\n",
      "(32, 3, 3, 32)\n",
      "(32, 3, 3, 32)\n",
      "./weights_layer4_tensorflow_new.hdf5\n",
      "(32, 3, 3, 1)\n",
      "(32, 3, 3, 32)\n",
      "(32, 3, 3, 32)\n",
      "(32, 3, 3, 32)\n",
      "(32, 3, 3, 32)\n",
      "./weights_layer3_tensorflow_new.hdf5\n",
      "(32, 3, 3, 1)\n",
      "(32, 3, 3, 32)\n",
      "(32, 3, 3, 32)\n",
      "(32, 3, 3, 32)\n"
     ]
    }
   ],
   "source": [
    "def copy_files(files):\n",
    "    for f in files:\n",
    "        convert_file(f, rename_file_newtf(f))\n",
    "copy_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.dirname(os.path.abspath('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/philkuz/research/test_repos/music-auto_tagging-keras'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
